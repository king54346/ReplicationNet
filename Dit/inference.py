import torch
import torch.nn.functional as F
from config import T
from dit import DiT
import matplotlib.pyplot as plt
from diffusion import *
from ray.train import Checkpoint
import numpy as np

DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'


def backward_denoise(model, x, y=None, cfg_scale=3.0, use_cfg=True):
    """
    DiTå»å™ªæ¨ç†è¿‡ç¨‹

    Args:
        model: DiTæ¨¡å‹
        x: [B, C, H, W] åˆå§‹å™ªå£°
        y: [B] ç±»åˆ«æ ‡ç­¾ (Noneè¡¨ç¤ºæ— æ¡ä»¶ç”Ÿæˆ)
        cfg_scale: Classifier-Free Guidanceå¼ºåº¦ (>1.0å¢å¼ºæ¡ä»¶æ§åˆ¶)
        use_cfg: æ˜¯å¦ä½¿ç”¨CFG

    Returns:
        steps: æ¯ä¸ªæ—¶é—´æ­¥çš„å›¾åƒåˆ—è¡¨
    """
    steps = [x.clone()]

    # å…¨å±€å˜é‡ç§»åˆ°è®¾å¤‡
    global alphas, alphas_cumprod, variance
    x = x.to(DEVICE)
    alphas = alphas.to(DEVICE)
    alphas_cumprod = alphas_cumprod.to(DEVICE)
    variance = variance.to(DEVICE)

    if y is not None:
        y = y.to(DEVICE)

    model.eval()
    with torch.no_grad():
        # ä»T-1åˆ°0é€æ­¥å»å™ª
        for time in range(T - 1, -1, -1):
            t = torch.full((x.size(0),), time, dtype=torch.long).to(DEVICE)

            # ===== 1. é¢„æµ‹å™ªå£° =====
            if use_cfg and y is not None:
                # Classifier-Free Guidance
                # åŒæ—¶é¢„æµ‹æ¡ä»¶å™ªå£°å’Œæ— æ¡ä»¶å™ªå£°
                noise_cond = model(x, t, y)  # æ¡ä»¶é¢„æµ‹
                noise_uncond = model(x, t, y=None)  # æ— æ¡ä»¶é¢„æµ‹

                # CFGå…¬å¼: noise = w * noise_cond + (1-w) * noise_uncond
                #         = noise_uncond + w * (noise_cond - noise_uncond)
                noise = noise_uncond + cfg_scale * (noise_cond - noise_uncond)
            else:
                # æ™®é€šé¢„æµ‹
                noise = model(x, t, y)

            # ===== 2. è®¡ç®—x_{t-1}çš„å‡å€¼ =====
            # æ ¹æ®DDPMå…¬å¼: Î¼_Î¸(x_t, t) = 1/âˆšÎ±_t * (x_t - (1-Î±_t)/âˆš(1-á¾±_t) * Îµ_Î¸)
            shape = (x.size(0), 1, 1, 1)

            alpha_t = alphas[t].view(*shape)  # Î±_t
            alpha_cumprod_t = alphas_cumprod[t].view(*shape)  # á¾±_t

            mean = (1.0 / torch.sqrt(alpha_t)) * (
                    x - ((1.0 - alpha_t) / torch.sqrt(1.0 - alpha_cumprod_t)) * noise
            )

            # ===== 3. æ·»åŠ éšæœºå™ªå£°(é™¤äº†æœ€åä¸€æ­¥) =====
            if time != 0:
                # Ïƒ_t = âˆšÎ²_t (æˆ–ä½¿ç”¨æ–¹å·®è°ƒåº¦)
                sigma_t = torch.sqrt(variance[t].view(*shape))
                z = torch.randn_like(x)  # æ ‡å‡†æ­£æ€å™ªå£°
                x = mean + sigma_t * z
            else:
                # t=0æ—¶ä¸æ·»åŠ å™ªå£°
                x = mean

            # ===== 4. è£å‰ªåˆ°æœ‰æ•ˆèŒƒå›´ =====
            x = torch.clamp(x, -1.0, 1.0)

            # ä¿å­˜å½“å‰æ­¥éª¤
            if time % (T // 20) == 0 or time == 0:  # æ¯5%ä¿å­˜ä¸€æ¬¡
                steps.append(x.clone().cpu())

            # æ‰“å°è¿›åº¦
            if time % 100 == 0:
                print(f"å»å™ªè¿›åº¦: {T - time}/{T} (å‰©ä½™{time}æ­¥)")

    return steps


def visualize_denoise_process(steps, save_path='denoise_process.png'):
    """
    å¯è§†åŒ–å»å™ªè¿‡ç¨‹

    Args:
        steps: backward_denoiseè¿”å›çš„æ­¥éª¤åˆ—è¡¨
        save_path: ä¿å­˜è·¯å¾„
    """
    batch_size = steps[0].size(0)
    num_steps = len(steps)

    fig, axes = plt.subplots(batch_size, num_steps, figsize=(num_steps * 2, batch_size * 2))

    for b in range(batch_size):
        for i, step_img in enumerate(steps):
            # åƒç´ å€¼ä»[-1,1]è¿˜åŸåˆ°[0,1]
            img = (step_img[b] + 1.0) / 2.0
            img = img.permute(1, 2, 0).numpy()  # [C,H,W] -> [H,W,C]
            img = np.clip(img, 0, 1)

            ax = axes[b, i] if batch_size > 1 else axes[i]
            ax.imshow(img, cmap='gray' if img.shape[2] == 1 else None)
            ax.axis('off')

            # ç¬¬ä¸€è¡Œæ˜¾ç¤ºæ­¥éª¤å·
            if b == 0:
                ax.set_title(f'Step {i}', fontsize=8)

    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"âœ… å¯è§†åŒ–ç»“æœå·²ä¿å­˜åˆ°: {save_path}")
    plt.show()


# ==================== ä¸»ç¨‹åº ====================
if __name__ == "__main__":
    print("=" * 50)
    print("DiT å»å™ªæ¨ç†è¿‡ç¨‹")
    print("=" * 50)

    # ===== 1. åŠ è½½æ¨¡å‹ =====
    print("\nğŸ“‚ åŠ è½½æ¨¡å‹...")
    checkpoint = Checkpoint.from_directory(
        "/home/user/demo/review/Dit/ray_results/dit_training/checkpoint_2026-01-06_18-04-49.906488"
    )

    with checkpoint.as_directory() as checkpoint_dir:
        checkpoint_data = torch.load(f"{checkpoint_dir}/checkpoint.pt")
        model = DiT(
            img_size=28,
            patch_size=4,
            in_channels=1,  # æ³¨æ„ä½ çš„ä»£ç é‡Œç”¨çš„æ˜¯channel,åº”è¯¥ç»Ÿä¸€
            hidden_dim=768,  # å¯¹åº”ä½ çš„emb_size
            depth=12,  # å¯¹åº”ä½ çš„dit_num
            num_heads=12,  # å¯¹åº”ä½ çš„head
            num_classes=10
        ).to(DEVICE)
        model.load_state_dict(checkpoint_data["model_state_dict"])

    print(f"âœ… æ¨¡å‹å·²åŠ è½½åˆ° {DEVICE}")

    # ===== 2. å‡†å¤‡è¾“å…¥ =====
    batch_size = 10

    # ç”Ÿæˆåˆå§‹å™ªå£° (ä»æ ‡å‡†æ­£æ€åˆ†å¸ƒé‡‡æ ·)
    x_T = torch.randn(batch_size, 1, 28, 28)

    # ç”Ÿæˆç±»åˆ«æ ‡ç­¾ (0-9å„ä¸€ä¸ª)
    y = torch.arange(0, 10, dtype=torch.long)

    print(f"\nğŸ² ç”Ÿæˆ {batch_size} ä¸ªåˆå§‹å™ªå£°")
    print(f"ğŸ“ ç±»åˆ«æ ‡ç­¾: {y.tolist()}")

    # ===== 3. æ‰§è¡Œå»å™ª =====
    print("\nğŸ”„ å¼€å§‹å»å™ªæ¨ç†...")
    print(f"æ€»æ­¥æ•°: {T}")
    print(f"CFGå¼ºåº¦: 3.0")

    steps = backward_denoise(
        model=model,
        x=x_T,
        y=y,
        cfg_scale=7.0,  # CFGå¼ºåº¦ (1.0=æ— CFG, 7.0=å¼ºæ¡ä»¶æ§åˆ¶)
        use_cfg=True  # å¯ç”¨CFG
    )

    print(f"\nâœ… å»å™ªå®Œæˆ! å…±ä¿å­˜ {len(steps)} ä¸ªä¸­é—´æ­¥éª¤")

    # ===== 4. å¯è§†åŒ–ç»“æœ =====
    print("\nğŸ“Š ç”Ÿæˆå¯è§†åŒ–...")
    visualize_denoise_process(steps, save_path='denoise_process.png')

    # ===== 5. ä¿å­˜æœ€ç»ˆç»“æœ =====
    final_images = steps[-1]  # [B, C, H, W]

    # å•ç‹¬ä¿å­˜æ¯ä¸ªæ•°å­—
    fig, axes = plt.subplots(1, 10, figsize=(20, 2))
    for i in range(10):
        img = (final_images[i] + 1.0) / 2.0
        img = img.permute(1, 2, 0).numpy()
        axes[i].imshow(img, cmap='gray')
        axes[i].set_title(f'Label: {i}', fontsize=12)
        axes[i].axis('off')

    plt.tight_layout()
    plt.savefig('final_results.png', dpi=150, bbox_inches='tight')
    print("âœ… æœ€ç»ˆç»“æœå·²ä¿å­˜åˆ°: final_results.png")
    plt.show()

    print("\n" + "=" * 50)
    print("æ¨ç†å®Œæˆ!")
    print("=" * 50)


# ==================== é¢å¤–åŠŸèƒ½ ====================

def generate_specific_digit(model, digit, num_samples=5, cfg_scale=3.0):
    """
    ç”ŸæˆæŒ‡å®šæ•°å­—çš„å¤šä¸ªæ ·æœ¬

    Args:
        model: DiTæ¨¡å‹
        digit: è¦ç”Ÿæˆçš„æ•°å­— (0-9)
        num_samples: ç”Ÿæˆæ•°é‡
        cfg_scale: CFGå¼ºåº¦
    """
    x_T = torch.randn(num_samples, 1, 28, 28)
    y = torch.full((num_samples,), digit, dtype=torch.long)

    print(f"\nç”Ÿæˆ {num_samples} ä¸ªæ•°å­— '{digit}' çš„æ ·æœ¬...")
    steps = backward_denoise(model, x_T, y, cfg_scale=cfg_scale)

    # å¯è§†åŒ–
    final = steps[-1]
    fig, axes = plt.subplots(1, num_samples, figsize=(num_samples * 2, 2))
    for i in range(num_samples):
        img = (final[i] + 1.0) / 2.0
        img = img.permute(1, 2, 0).numpy()
        ax = axes[i] if num_samples > 1 else axes
        ax.imshow(img, cmap='gray')
        ax.axis('off')

    plt.suptitle(f'Generated Digit: {digit}', fontsize=14)
    plt.tight_layout()
    plt.savefig(f'digit_{digit}_samples.png', dpi=150)
    plt.show()


def compare_cfg_strengths(model, digit=3):
    """
    æ¯”è¾ƒä¸åŒCFGå¼ºåº¦çš„æ•ˆæœ
    """
    cfg_scales = [1.0, 2.0, 3.0, 5.0, 7.0]

    fig, axes = plt.subplots(1, len(cfg_scales), figsize=(len(cfg_scales) * 2, 2))

    for i, scale in enumerate(cfg_scales):
        x_T = torch.randn(1, 1, 28, 28)
        y = torch.tensor([digit])

        steps = backward_denoise(model, x_T, y, cfg_scale=scale)
        img = (steps[-1][0] + 1.0) / 2.0
        img = img.permute(1, 2, 0).numpy()

        axes[i].imshow(img, cmap='gray')
        axes[i].set_title(f'CFG={scale}', fontsize=10)
        axes[i].axis('off')

    plt.suptitle(f'CFG Scale Comparison (Digit {digit})', fontsize=14)
    plt.tight_layout()
    plt.savefig('cfg_comparison.png', dpi=150)
    plt.show()